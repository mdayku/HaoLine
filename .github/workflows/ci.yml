# HaoLine CI Pipeline
# Runs linting, type checking, and unit tests

name: CI

on:
  push:
    branches: [main]
    tags: ['v*']  # Trigger on version tags for PyPI publish
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    name: Lint & Format
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy black
          pip install onnx numpy

      - name: Check formatting with Black
        run: |
          black --check --diff src/haoline/

      - name: Lint with Ruff
        run: |
          ruff check src/haoline/

      - name: Type check with mypy
        run: |
          mypy src/haoline/ \
            --ignore-missing-imports \
            --no-error-summary \
            --show-error-codes \
            || echo "::warning::Type checking found issues (non-blocking)"
        continue-on-error: true

  test:
    name: Tests (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install package with dev dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          pytest src/haoline/tests/ -v --tb=short --color=yes

      - name: Run tests with coverage
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        run: |
          pytest src/haoline/tests/ --cov=haoline --cov-report=term-missing --cov-report=xml

      - name: Upload coverage report
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          flags: haoline
          fail_ci_if_error: false
        continue-on-error: true

  integration:
    name: Integration Test
    runs-on: ubuntu-latest
    needs: [lint, test]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install package
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[viz]"

      - name: Create test model
        run: |
          python -c "
          import numpy as np
          import onnx
          from onnx import TensorProto, helper

          # Create a simple Conv-BN-Relu model
          X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [1, 3, 224, 224])
          W = helper.make_tensor('W', TensorProto.FLOAT, [64, 3, 7, 7],
                                 np.random.randn(64, 3, 7, 7).astype(np.float32).flatten().tolist())
          scale = helper.make_tensor('scale', TensorProto.FLOAT, [64], np.ones(64, dtype=np.float32).tolist())
          bias = helper.make_tensor('bias', TensorProto.FLOAT, [64], np.zeros(64, dtype=np.float32).tolist())
          mean = helper.make_tensor('mean', TensorProto.FLOAT, [64], np.zeros(64, dtype=np.float32).tolist())
          var = helper.make_tensor('var', TensorProto.FLOAT, [64], np.ones(64, dtype=np.float32).tolist())
          Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1, 64, 112, 112])

          conv = helper.make_node('Conv', ['X', 'W'], ['conv_out'], kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3])
          bn = helper.make_node('BatchNormalization', ['conv_out', 'scale', 'bias', 'mean', 'var'], ['bn_out'])
          relu = helper.make_node('Relu', ['bn_out'], ['Y'])

          graph = helper.make_graph([conv, bn, relu], 'test_model', [X], [Y], [W, scale, bias, mean, var])
          model = helper.make_model(graph, opset_imports=[helper.make_opsetid('', 17)])
          onnx.save(model, 'test_model.onnx')
          print('Created test_model.onnx')
          "

      - name: Test CLI - Basic inspection
        run: |
          haoline test_model.onnx

      - name: Test CLI - JSON output
        run: |
          haoline test_model.onnx --out-json report.json
          cat report.json | head -50

      - name: Test CLI - Markdown output
        run: |
          haoline test_model.onnx --out-md model_card.md
          cat model_card.md

      - name: Test CLI - Hardware estimates
        run: |
          haoline test_model.onnx --hardware a100 --precision fp16 --batch-size 8

      - name: Test CLI - List hardware profiles
        run: |
          haoline --list-hardware

      - name: Test CLI - HTML report with plots
        run: |
          haoline test_model.onnx --out-html report.html --with-plots

      - name: Verify outputs exist
        run: |
          test -f report.json
          test -f model_card.md
          test -f report.html
          echo "All integration tests passed!"

  # Deploy to HuggingFace Spaces (after tests pass, or after PyPI publish on tags)
  deploy-hf:
    name: Deploy to HF Spaces
    runs-on: ubuntu-latest
    needs: [lint, test, integration, publish]
    # Run if: (main branch push) OR (tag push after successful publish)
    if: |
      always() &&
      needs.integration.result == 'success' &&
      (
        (github.ref == 'refs/heads/main') ||
        (startsWith(github.ref, 'refs/tags/v') && needs.publish.result == 'success')
      )

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check if HF_TOKEN is set
        id: check_token
        run: |
          if [ -z "${{ secrets.HF_TOKEN }}" ]; then
            echo "HF_TOKEN not set, skipping deployment"
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Install huggingface_hub
        if: steps.check_token.outputs.skip != 'true'
        run: pip install huggingface_hub

      - name: Create Space files
        if: steps.check_token.outputs.skip != 'true'
        run: |
          mkdir -p hf_space

          # Create Dockerfile
          cat > hf_space/Dockerfile << 'EOF'
          FROM python:3.11-slim

          WORKDIR /app

          RUN apt-get update && apt-get install -y \
              build-essential \
              git \
              && rm -rf /var/lib/apt/lists/*

          RUN pip install --no-cache-dir "haoline[web]>=0.2.4"

          COPY app.py .
          COPY .streamlit/config.toml .streamlit/config.toml

          EXPOSE 7860

          CMD ["python", "-m", "streamlit", "run", "app.py", "--server.port=7860", "--server.address=0.0.0.0", "--server.headless=true", "--server.enableXsrfProtection=false", "--server.enableCORS=false", "--server.maxUploadSize=500"]
          EOF

          # Create app.py
          cat > hf_space/app.py << 'EOF'
          """HaoLine - Universal Model Inspector"""
          from haoline.streamlit_app import main
          main()
          EOF

          # Create Streamlit config for dark theme
          mkdir -p hf_space/.streamlit
          cat > hf_space/.streamlit/config.toml << 'EOF'
          [theme]
          base = "dark"
          primaryColor = "#10b981"
          backgroundColor = "#0d0d0d"
          secondaryBackgroundColor = "#161616"
          textColor = "#f5f5f5"

          [server]
          maxUploadSize = 500
          EOF

          # Create README
          cat > hf_space/README.md << 'EOF'
          ---
          title: HaoLine
          emoji: ðŸ”¬
          colorFrom: green
          colorTo: blue
          sdk: docker
          pinned: false
          license: mit
          app_port: 7860
          ---

          # HaoLine (çš“çº¿) - Universal Model Inspector

          Upload ONNX/PyTorch models for architecture analysis, interactive visualization, and AI-powered summaries.

          **Links:** [GitHub](https://github.com/mdayku/HaoLine) | [PyPI](https://pypi.org/project/haoline/)
          EOF

      - name: Push to Hugging Face Space
        if: steps.check_token.outputs.skip != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          cd hf_space
          git init -b main
          git config user.email "github-actions@github.com"
          git config user.name "GitHub Actions"
          git add .
          git commit -m "Deploy from GitHub Actions"
          git push --force https://mdayku:${HF_TOKEN}@huggingface.co/spaces/mdayku/haoline main

      - name: Deployment status
        if: steps.check_token.outputs.skip != 'true'
        run: echo "âœ… Deployed to https://huggingface.co/spaces/mdayku/haoline"

  # Auto-publish to PyPI on release
  publish:
    name: Publish to PyPI
    runs-on: ubuntu-latest
    needs: [lint, test, integration]
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install build tools
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build package
        run: python -m build

      - name: Publish to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: twine upload dist/*
